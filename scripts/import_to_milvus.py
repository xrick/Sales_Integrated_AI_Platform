#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°‡ç­†é›»è¦æ ¼è³‡æ–™åŒ¯å…¥åˆ°Milvus
ä½¿ç”¨Parent-Child ChunkingæŠ€è¡“
"""

import os
import csv
import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection
from sentence_transformers import SentenceTransformer

# å°å…¥chunkingå¼•æ“
from parent_child_chunking import NotebookParentChildChunker

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/milvus_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é…ç½®
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "new_nb_pc_v1"
CSV_DIR = "data/raw/EM_New TTL_241104_AllTransformedToGoogleSheet"
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
DIMENSION = 384

class MilvusNotebookImporter:
    """Milvusç­†é›»è¦æ ¼è³‡æ–™åŒ¯å…¥å™¨"""
    
    def __init__(self):
        self.connection = None
        self.collection = None
        self.embedding_model = None
        self.chunker = NotebookParentChildChunker()
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'files_processed': 0,
            'total_products': 0,
            'total_chunks': 0,
            'parent_chunks': 0,
            'child_chunks': 0,
            'milvus_inserts': 0,
            'errors': 0
        }
    
    def connect_to_milvus(self):
        """é€£æ¥åˆ°Milvus"""
        try:
            logger.info(f"é€£æ¥åˆ°Milvus: {MILVUS_HOST}:{MILVUS_PORT}")
            connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
            logger.info("âœ… æˆåŠŸé€£æ¥åˆ°Milvus")
            return True
        except Exception as e:
            logger.error(f"âŒ é€£æ¥Milvuså¤±æ•—: {e}")
            return False
    
    def setup_collection(self):
        """å»ºç«‹æˆ–è¨­å®šcollection"""
        try:
            # æª¢æŸ¥collectionæ˜¯å¦å­˜åœ¨
            if utility.has_collection(COLLECTION_NAME):
                logger.info(f"Collection '{COLLECTION_NAME}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆªé™¤...")
                utility.drop_collection(COLLECTION_NAME)
            
            # å®šç¾©schema
            fields = [
                FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="doc_id", dtype=DataType.INT64),
                FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=512),
                FieldSchema(name="parent_text", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=1000),
                FieldSchema(name="query_vector", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION),
                FieldSchema(name="chunk_type", dtype=DataType.VARCHAR, max_length=50),
                FieldSchema(name="product_id", dtype=DataType.VARCHAR, max_length=256),
                FieldSchema(name="modeltype", dtype=DataType.VARCHAR, max_length=100)
            ]
            
            schema = CollectionSchema(fields, "Notebook specs with parent-child chunking")
            
            # å»ºç«‹collection
            logger.info(f"å»ºç«‹collection: {COLLECTION_NAME}")
            self.collection = Collection(COLLECTION_NAME, schema)
            
            # å»ºç«‹ç´¢å¼•
            logger.info("å»ºç«‹å‘é‡ç´¢å¼•...")
            index_params = {
                "metric_type": "L2",
                "index_type": "IVF_FLAT",
                "params": {"nlist": 1024}
            }
            self.collection.create_index("query_vector", index_params)
            
            logger.info("âœ… Collectionå»ºç«‹å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å»ºç«‹collectionå¤±æ•—: {e}")
            return False
    
    def load_embedding_model(self):
        """è¼‰å…¥embeddingæ¨¡å‹"""
        try:
            logger.info(f"è¼‰å…¥embeddingæ¨¡å‹: {EMBEDDING_MODEL}")
            self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
            logger.info("âœ… Embeddingæ¨¡å‹è¼‰å…¥å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥embeddingæ¨¡å‹å¤±æ•—: {e}")
            return False
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            if not self.embedding_model:
                raise ValueError("Embeddingæ¨¡å‹æœªè¼‰å…¥")
            
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = text.strip()
            if not cleaned_text:
                return [0.0] * DIMENSION
            
            # ç”Ÿæˆembedding
            embedding = self.embedding_model.encode(cleaned_text)
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆembeddingå¤±æ•—: {e}")
            return [0.0] * DIMENSION
    
    def process_csv_file(self, csv_file: Path) -> List[Dict[str, Any]]:
        """è™•ç†å–®å€‹CSVæª”æ¡ˆ"""
        try:
            logger.info(f"è™•ç†æª”æ¡ˆ: {csv_file.name}")
            
            # è®€å–CSV
            df = pd.read_csv(csv_file, dtype={'modeltype': str})
            
            # æ·»åŠ ä¾†æºæª”æ¡ˆè³‡è¨Š
            df['source_file'] = csv_file.name
            
            # è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
            products = df.to_dict('records')
            
            logger.info(f"æª”æ¡ˆ {csv_file.name} åŒ…å« {len(products)} å€‹ç”¢å“")
            return products
            
        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆ {csv_file.name} å¤±æ•—: {e}")
            return []
    
    def create_chunks_with_embeddings(self, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç‚ºç”¢å“å»ºç«‹chunksä¸¦ç”Ÿæˆembeddings"""
        try:
            logger.info(f"é–‹å§‹ç‚º {len(products)} å€‹ç”¢å“å»ºç«‹chunks")
            
            # ä½¿ç”¨chunkingå¼•æ“å»ºç«‹chunks
            parent_chunks, child_chunks = self.chunker.batch_create_chunks(products)
            
            all_chunks = []
            chunk_id_counter = 0
            
            # è™•ç†parent chunks
            for parent_chunk in parent_chunks:
                chunk_id_counter += 1
                
                # ç”Ÿæˆembedding
                embedding = self.generate_embedding(parent_chunk['content'])
                
                # æº–å‚™Milvusæ’å…¥è³‡æ–™
                milvus_chunk = {
                    'doc_id': chunk_id_counter,
                    'source': parent_chunk.get('source', ''),
                    'parent_text': parent_chunk['content'],
                    'chunk_text': parent_chunk['content'],
                    'query_vector': embedding,
                    'chunk_type': 'parent',
                    'product_id': parent_chunk.get('product_id', ''),
                    'modeltype': parent_chunk.get('modeltype', '')
                }
                
                all_chunks.append(milvus_chunk)
                self.stats['parent_chunks'] += 1
            
            # è™•ç†child chunks
            for child_chunk in child_chunks:
                if not child_chunk or not child_chunk.get('content'):
                    continue
                
                chunk_id_counter += 1
                
                # ç”Ÿæˆembedding
                embedding = self.generate_embedding(child_chunk['content'])
                
                # æº–å‚™Milvusæ’å…¥è³‡æ–™
                milvus_chunk = {
                    'doc_id': chunk_id_counter,
                    'source': child_chunk.get('source', ''),
                    'parent_text': '',  # child chunksæ²’æœ‰parent_text
                    'chunk_text': child_chunk['content'],
                    'query_vector': embedding,
                    'chunk_type': child_chunk['chunk_type'],
                    'product_id': child_chunk.get('product_id', ''),
                    'modeltype': child_chunk.get('modeltype', '')
                }
                
                all_chunks.append(milvus_chunk)
                self.stats['child_chunks'] += 1
            
            self.stats['total_chunks'] = len(all_chunks)
            logger.info(f"æˆåŠŸå»ºç«‹ {len(all_chunks)} å€‹chunks")
            
            return all_chunks
            
        except Exception as e:
            logger.error(f"å»ºç«‹chunkså¤±æ•—: {e}")
            return []
    
    def insert_to_milvus(self, chunks: List[Dict[str, Any]]):
        """å°‡chunksæ’å…¥åˆ°Milvus"""
        try:
            if not chunks:
                logger.warning("æ²’æœ‰chunkséœ€è¦æ’å…¥")
                return
            
            logger.info(f"é–‹å§‹æ’å…¥ {len(chunks)} å€‹chunksåˆ°Milvus")
            
            # æº–å‚™æ’å…¥è³‡æ–™ - æŒ‰ç…§schemaæ¬„ä½é †åº
            doc_ids = []
            sources = []
            parent_texts = []
            chunk_texts = []
            query_vectors = []
            chunk_types = []
            product_ids = []
            modeltypes = []
            
            for chunk in chunks:
                doc_ids.append(chunk['doc_id'])
                sources.append(chunk['source'])
                parent_texts.append(chunk['parent_text'])
                chunk_texts.append(chunk['chunk_text'])
                query_vectors.append(chunk['query_vector'])
                chunk_types.append(chunk['chunk_type'])
                product_ids.append(chunk['product_id'])
                modeltypes.append(chunk['modeltype'])
            
            # æŒ‰ç…§schemaæ¬„ä½é †åºçµ„ç¹”è³‡æ–™
            insert_data = [
                doc_ids,
                sources,
                parent_texts,
                chunk_texts,
                query_vectors,
                chunk_types,
                product_ids,
                modeltypes
            ]
            
            # æ’å…¥è³‡æ–™
            try:
                self.collection.insert(insert_data)
                logger.info(f"æˆåŠŸæ’å…¥ {len(chunks)} å€‹chunksåˆ°Milvus")
                self.stats['milvus_inserts'] += len(chunks)
            except Exception as e:
                logger.error(f"æ’å…¥åˆ°Milvuså¤±æ•—: {e}")
                self.stats['errors'] += len(chunks)
                raise
            
            logger.info("âœ… æ‰€æœ‰chunksæ’å…¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ’å…¥åˆ°Milvuså¤±æ•—: {e}")
            raise
    
    def process_all_files(self):
        """è™•ç†æ‰€æœ‰CSVæª”æ¡ˆ"""
        try:
            # ç¢ºä¿logsç›®éŒ„å­˜åœ¨
            os.makedirs('logs', exist_ok=True)
            
            # é€£æ¥åˆ°Milvus
            if not self.connect_to_milvus():
                return False
            
            # è¨­å®šcollection
            if not self.setup_collection():
                return False
            
            # è¼‰å…¥embeddingæ¨¡å‹
            if not self.load_embedding_model():
                return False
            
            # ç²å–æ‰€æœ‰CSVæª”æ¡ˆ
            csv_files = list(Path(CSV_DIR).glob('*.csv'))
            csv_files.sort()
            
            if not csv_files:
                logger.error(f"åœ¨ç›®éŒ„ {CSV_DIR} ä¸­æœªæ‰¾åˆ°CSVæª”æ¡ˆ")
                return False
            
            logger.info(f"æ‰¾åˆ° {len(csv_files)} å€‹CSVæª”æ¡ˆ")
            
            # è™•ç†æ¯å€‹æª”æ¡ˆ
            all_products = []
            for csv_file in csv_files:
                try:
                    products = self.process_csv_file(csv_file)
                    all_products.extend(products)
                    self.stats['files_processed'] += 1
                    
                except Exception as e:
                    logger.error(f"è™•ç†æª”æ¡ˆ {csv_file.name} å¤±æ•—: {e}")
                    continue
            
            self.stats['total_products'] = len(all_products)
            logger.info(f"ç¸½å…±è™•ç† {len(all_products)} å€‹ç”¢å“")
            
            # å»ºç«‹chunks
            chunks = self.create_chunks_with_embeddings(all_products)
            
            # æ’å…¥åˆ°Milvus
            self.insert_to_milvus(chunks)
            
            # è¼‰å…¥collectionåˆ°è¨˜æ†¶é«”
            logger.info("è¼‰å…¥collectionåˆ°è¨˜æ†¶é«”...")
            self.collection.load()
            
            # åˆ·æ–°è³‡æ–™
            logger.info("åˆ·æ–°collectionè³‡æ–™...")
            self.collection.flush()
            
            logger.info("âœ… æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆå¤±æ•—: {e}")
            return False
        
        finally:
            # æ¸…ç†é€£æ¥
            if self.connection:
                connections.disconnect("default")
                logger.info("å·²æ–·é–‹Milvusé€£æ¥")
    
    def get_stats(self) -> Dict[str, Any]:
        """å–å¾—è™•ç†çµ±è¨ˆè³‡è¨Š"""
        return self.stats.copy()
    
    def print_stats(self):
        """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
        stats = self.get_stats()
        logger.info("=" * 60)
        logger.info("ğŸ“Š è™•ç†çµ±è¨ˆè³‡è¨Š:")
        logger.info(f"è™•ç†æª”æ¡ˆæ•¸: {stats['files_processed']}")
        logger.info(f"ç¸½ç”¢å“æ•¸: {stats['total_products']}")
        logger.info(f"ç¸½Chunksæ•¸: {stats['total_chunks']}")
        logger.info(f"Parent Chunks: {stats['parent_chunks']}")
        logger.info(f"Child Chunks: {stats['child_chunks']}")
        logger.info(f"Milvusæ’å…¥æ•¸: {stats['milvus_inserts']}")
        logger.info(f"éŒ¯èª¤æ•¸: {stats['errors']}")
        logger.info("=" * 60)

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œç­†é›»è¦æ ¼è³‡æ–™åŒ¯å…¥åˆ°Milvus")
    logger.info("=" * 60)
    
    importer = MilvusNotebookImporter()
    
    try:
        success = importer.process_all_files()
        
        if success:
            logger.info("ğŸ‰ è³‡æ–™åŒ¯å…¥æˆåŠŸå®Œæˆï¼")
            importer.print_stats()
        else:
            logger.error("âŒ è³‡æ–™åŒ¯å…¥å¤±æ•—")
            
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}")
        raise

if __name__ == "__main__":
    main()
