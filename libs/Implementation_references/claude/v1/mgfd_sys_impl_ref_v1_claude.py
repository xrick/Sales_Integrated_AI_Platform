import asyncio
import uuid
import time
from typing import Dict, Any, List
from dataclasses import asdict

# =====================================
# MGFDå®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼šCase-1å¯¦ç¾
# =====================================

class MGFDWorkflowExample:
    """MGFDç³»çµ±å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹"""
    
    def __init__(self):
        self.kernel = MGFDKernel()
        self.session_contexts = {}
    
    async def handle_user_request(self, user_input: str, session_id: str) -> Dict[str, Any]:
        """è™•ç†ç”¨æˆ¶è«‹æ±‚çš„å®Œæ•´æµç¨‹"""
        
        # æ­¥é©Ÿ1ï¼šåˆå§‹åŒ–æœƒè©±ä¸Šä¸‹æ–‡
        if session_id not in self.session_contexts:
            self.session_contexts[session_id] = self.kernel.create_standard_context(session_id)
        
        context = self.session_contexts[session_id]
        context["raw_user_input"] = user_input
        context["workflow_trace"] = []  # ç”¨æ–¼è¿½è¹¤æ•´å€‹æµç¨‹
        
        try:
            # æ­¥é©Ÿ2ï¼šç”¨æˆ¶è¼¸å…¥åˆ†æ
            analysis_result = await self._step_user_input_analysis(context)
            if analysis_result.status != ActionStatus.SUCCESS:
                return self._create_error_response("è¼¸å…¥åˆ†æå¤±æ•—", analysis_result.message)
            
            # æ­¥é©Ÿ3ï¼šæ„åœ–è­˜åˆ¥å’Œæ§½ä½æå–
            intent_result = await self._step_intent_extraction(context) 
            if intent_result.status != ActionStatus.SUCCESS:
                return self._create_error_response("æ„åœ–è­˜åˆ¥å¤±æ•—", intent_result.message)
            
            # æ­¥é©Ÿ4ï¼šæª¢æŸ¥æ˜¯å¦éœ€è¦æ§½ä½è£œå……
            if self._needs_slot_filling(context):
                return await self._step_slot_filling_conversation(context)
            
            # æ­¥é©Ÿ5ï¼šçŸ¥è­˜åº«æœç´¢
            search_result = await self._step_knowledge_search(context)
            if search_result.status != ActionStatus.SUCCESS:
                return self._create_error_response("çŸ¥è­˜æœç´¢å¤±æ•—", search_result.message)
            
            # æ­¥é©Ÿ6ï¼šæç¤ºå·¥ç¨‹å’ŒéŸ¿æ‡‰ç”Ÿæˆ
            response_result = await self._step_response_generation(context)
            if response_result.status != ActionStatus.SUCCESS:
                return self._create_error_response("éŸ¿æ‡‰ç”Ÿæˆå¤±æ•—", response_result.message)
            
            # æ­¥é©Ÿ7ï¼šç‹€æ…‹æ›´æ–°
            await self._step_state_update(context)
            
            return {
                "success": True,
                "response": context.get("final_response"),
                "ui_elements": context.get("ui_elements"),
                "session_id": session_id,
                "workflow_trace": context["workflow_trace"]
            }
            
        except Exception as e:
            return self._create_error_response("ç³»çµ±éŒ¯èª¤", str(e))
    
    async def _step_user_input_analysis(self, context: Dict[str, Any]) -> ActionResult:
        """æ­¥é©Ÿ1ï¼šç”¨æˆ¶è¼¸å…¥åˆ†æ"""
        start_time = time.time()
        context["workflow_trace"].append("é–‹å§‹ç”¨æˆ¶è¼¸å…¥åˆ†æ")
        
        # å‰µå»ºUserInputHandlerè«‹æ±‚æ¶ˆæ¯
        request_payload = UserInputHandlerProtocol.create_input_analysis_request(
            context["raw_user_input"], 
            context["session_id"]
        )
        
        message = MGFDMessage(
            message_id=str(uuid.uuid4()),
            message_type=MGFDMessageType.USER_INPUT_REQUEST,
            sender_module="MGFDKernel",
            receiver_module="UserInputHandler", 
            timestamp=time.time(),
            payload=request_payload,
            correlation_id=context["session_id"]
        )
        
        # æ¨¡æ“¬UserInputHandleréŸ¿æ‡‰
        mock_response = {
            "status": "success",
            "data": {
                "parsed_input": context["raw_user_input"],
                "language": "zh-TW",
                "input_type": "product_inquiry",
                "preprocessing_applied": ["normalization", "tokenization"]
            }
        }
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context.update(mock_response["data"])
        context["workflow_trace"].append(f"è¼¸å…¥åˆ†æå®Œæˆ - {time.time() - start_time:.3f}s")
        
        return ActionResult(
            status=ActionStatus.SUCCESS,
            data=mock_response["data"],
            execution_time=time.time() - start_time
        )
    
    async def _step_intent_extraction(self, context: Dict[str, Any]) -> ActionResult:
        """æ­¥é©Ÿ2ï¼šæ„åœ–æå–å’Œå¯¦é«”è­˜åˆ¥"""
        start_time = time.time()
        context["workflow_trace"].append("é–‹å§‹æ„åœ–æå–")
        
        # æ¨¡æ“¬NLPè™•ç†çµæœ
        mock_nlp_result = {
            "intent": "laptop_recommendation_request",
            "confidence": 0.92,
            "entities": [
                {"type": "PRODUCT", "value": "ç­†é›»", "start": 6, "end": 8},
                {"type": "REQUIREMENT", "value": "æ–°å‡ºçš„", "start": 2, "end": 5}
            ],
            "extracted_slots": {
                "product_category": "ç­†é›»",
                "requirement_type": "æœ€æ–°ç”¢å“",
                "budget": None,  # éœ€è¦å¾ŒçºŒæ”¶é›†
                "usage_scenario": None  # éœ€è¦å¾ŒçºŒæ”¶é›†
            }
        }
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context.update(mock_nlp_result)
        context["workflow_trace"].append(f"æ„åœ–æå–å®Œæˆ - æ„åœ–: {mock_nlp_result['intent']}")
        
        return ActionResult(
            status=ActionStatus.SUCCESS,
            data=mock_nlp_result,
            execution_time=time.time() - start_time
        )
    
    def _needs_slot_filling(self, context: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦æ§½ä½è£œå……"""
        slots = context.get("extracted_slots", {})
        required_slots = ["product_category", "budget", "usage_scenario"]
        
        missing_slots = [slot for slot in required_slots if not slots.get(slot)]
        context["missing_slots"] = missing_slots
        
        return len(missing_slots) > 0
    
    async def _step_slot_filling_conversation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥é©Ÿ3ï¼šæ§½ä½è£œå……å°è©±"""
        context["workflow_trace"].append("é€²å…¥æ§½ä½è£œå……å°è©±")
        
        missing_slots = context["missing_slots"]
        next_slot = missing_slots[0]
        
        # ç”Ÿæˆæ§½ä½æ”¶é›†å•é¡Œ
        slot_questions = {
            "budget": "è«‹å•æ‚¨çš„é ç®—ç¯„åœæ˜¯å¤šå°‘ï¼Ÿ",
            "usage_scenario": "è«‹å•æ‚¨ä¸»è¦ç”¨é€™å°ç­†é›»åšä»€éº¼ç”¨é€”ï¼Ÿï¼ˆä¾‹å¦‚ï¼šè¾¦å…¬ã€éŠæˆ²ã€è¨­è¨ˆç­‰ï¼‰"
        }
        
        question = slot_questions.get(next_slot, f"è«‹æä¾› {next_slot} çš„ç›¸é—œä¿¡æ¯")
        
        return {
            "success": True,
            "response": question,
            "ui_elements": {
                "type": "slot_collection",
                "required_slot": next_slot,
                "progress": {
                    "completed": len(context["extracted_slots"]) - len(missing_slots),
                    "total": len(context["extracted_slots"])
                }
            },
            "session_id": context["session_id"],
            "requires_user_response": True
        }
    
    async def _step_knowledge_search(self, context: Dict[str, Any]) -> ActionResult:
        """æ­¥é©Ÿ4ï¼šçŸ¥è­˜åº«èªç¾©æœç´¢"""
        start_time = time.time()
        context["workflow_trace"].append("é–‹å§‹çŸ¥è­˜åº«æœç´¢")
        
        # æ§‹å»ºæœç´¢è«‹æ±‚
        search_request = KnowledgeManagementHandlerProtocol.create_chunking_search_request(
            query=f"æ¨è–¦{context['extracted_slots']['product_category']}",
            slots=context["extracted_slots"]
        )
        
        # æ¨¡æ“¬Parent-Child Chunkingæœç´¢çµæœ
        mock_search_results = [
            {
                "product_id": "LAPTOP_2024_001",
                "product_name": "MacBook Pro 14\" M3",
                "similarity_score": 0.94,
                "chunk_info": {
                    "chunk_type": "parent",
                    "chunk_id": "PARENT_LAPTOP_001",
                    "child_chunks": ["CHILD_PERFORMANCE_001", "CHILD_DESIGN_001"]
                },
                "matching_reasons": [
                    "ç¬¦åˆæœ€æ–°ç”¢å“è¦æ±‚",
                    "é«˜æ•ˆèƒ½è™•ç†å™¨",
                    "å°ˆæ¥­ç´šè¨­è¨ˆ"
                ],
                "specifications": {
                    "processor": "Apple M3 Pro",
                    "memory": "18GB",
                    "storage": "512GB SSD",
                    "display": "14.2å‹ Liquid Retina XDR",
                    "price": "NT$ 69,900"
                }
            },
            {
                "product_id": "LAPTOP_2024_002", 
                "product_name": "ASUS ROG Zephyrus G14",
                "similarity_score": 0.89,
                "chunk_info": {
                    "chunk_type": "child", 
                    "chunk_id": "CHILD_GAMING_002",
                    "parent_chunk": "PARENT_LAPTOP_002"
                },
                "matching_reasons": [
                    "2024å¹´æ–°æ¬¾",
                    "éŠæˆ²æ•ˆèƒ½å„ªç•°", 
                    "ä¾¿æ”œæ€§ä½³"
                ],
                "specifications": {
                    "processor": "AMD Ryzen 9 7940HS",
                    "memory": "32GB",
                    "storage": "1TB SSD",
                    "graphics": "NVIDIA RTX 4060",
                    "price": "NT$ 59,900"
                }
            }
        ]
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context["search_results"] = mock_search_results
        context["total_matches"] = len(mock_search_results)
        context["workflow_trace"].append(f"æœç´¢å®Œæˆ - æ‰¾åˆ° {len(mock_search_results)} å€‹ç›¸é—œç”¢å“")
        
        return ActionResult(
            status=ActionStatus.SUCCESS,
            data={"search_results": mock_search_results},
            execution_time=time.time() - start_time
        )
    
    async def _step_response_generation(self, context: Dict[str, Any]) -> ActionResult:
        """æ­¥é©Ÿ5ï¼šéŸ¿æ‡‰ç”Ÿæˆ"""
        start_time = time.time()
        context["workflow_trace"].append("é–‹å§‹éŸ¿æ‡‰ç”Ÿæˆ")
        
        # é¸æ“‡é©ç•¶çš„æç¤ºæ¨¡æ¿
        prompt_request = PromptManagementHandlerProtocol.create_prompt_selection_request(
            context=context,
            intent=context["intent"]
        )
        
        # æ¨¡æ“¬é¸ä¸­çš„æç¤ºæ¨¡æ¿
        selected_prompt = """
        æ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚ï¼Œç‚ºä»¥ä¸‹ç­†é›»ç”¢å“ç”Ÿæˆå°ˆæ¥­çš„æ¨è–¦èªªæ˜ï¼š
        - é‡é»çªå‡ºç”¢å“ç‰¹è‰²å’Œå„ªå‹¢
        - èªªæ˜ç‚ºä»€éº¼é©åˆç”¨æˆ¶éœ€æ±‚  
        - æä¾›æ¸…æ™°çš„è¦æ ¼å°æ¯”
        - èªèª¿è¦å°ˆæ¥­å‹å–„
        """
        
        # ç”ŸæˆéŸ¿æ‡‰å…§å®¹
        response_content = self._generate_product_recommendation_response(
            context["search_results"], 
            context["extracted_slots"]
        )
        
        # ç”ŸæˆUIå…ƒç´ 
        ui_elements = {
            "type": "product_recommendation",
            "products": [
                {
                    "id": result["product_id"],
                    "name": result["product_name"],
                    "image_url": f"/images/{result['product_id']}.jpg",
                    "key_specs": result["specifications"],
                    "similarity_score": result["similarity_score"],
                    "recommendation_reasons": result["matching_reasons"]
                }
                for result in context["search_results"]
            ],
            "follow_up_questions": [
                "éœ€è¦äº†è§£æ›´è©³ç´°çš„è¦æ ¼å—ï¼Ÿ",
                "æƒ³çœ‹å…¶ä»–åƒ¹æ ¼å€é–“çš„é¸æ“‡å—ï¼Ÿ",
                "æœ‰ä»€éº¼å…·é«”çš„ä½¿ç”¨éœ€æ±‚è¦è¨è«–ï¼Ÿ"
            ]
        }
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context["final_response"] = response_content
        context["ui_elements"] = ui_elements
        context["workflow_trace"].append(f"éŸ¿æ‡‰ç”Ÿæˆå®Œæˆ - {time.time() - start_time:.3f}s")
        
        return ActionResult(
            status=ActionStatus.SUCCESS,
            data={"response": response_content, "ui_elements": ui_elements},
            execution_time=time.time() - start_time
        )
    
    def _generate_product_recommendation_response(self, search_results: List[Dict], slots: Dict[str, Any]) -> str:
        """ç”Ÿæˆç”¢å“æ¨è–¦éŸ¿æ‡‰å…§å®¹"""
        response = "æ ¹æ“šæ‚¨å°æœ€æ–°ç­†é›»çš„éœ€æ±‚ï¼Œæˆ‘ç‚ºæ‚¨æ¨è–¦ä»¥ä¸‹å¹¾æ¬¾ç”¢å“ï¼š\n\n"
        
        for i, product in enumerate(search_results, 1):
            response += f"**{i}. {product['product_name']}**\n"
            response += f"åƒ¹æ ¼ï¼š{product['specifications']['price']}\n"
            response += f"æ¨è–¦ç†ç”±ï¼š{', '.join(product['matching_reasons'])}\n"
            response += f"ç›¸ä¼¼åº¦ï¼š{product['similarity_score']:.1%}\n\n"
            
            response += "ä¸»è¦è¦æ ¼ï¼š\n"
            for key, value in product['specifications'].items():
                if key != 'price':
                    response += f"- {key.title()}: {value}\n"
            response += "\n---\n\n"
        
        response += "é€™äº›ç”¢å“éƒ½æ˜¯2024å¹´çš„æœ€æ–°æ¬¾ï¼Œæ‚¨å¯ä»¥æ ¹æ“šé ç®—å’Œä½¿ç”¨éœ€æ±‚é¸æ“‡ã€‚éœ€è¦æˆ‘è©³ç´°ä»‹ç´¹å“ªä¸€æ¬¾å—ï¼Ÿ"
        
        return response
    
    async def _step_state_update(self, context: Dict[str, Any]) -> ActionResult:
        """æ­¥é©Ÿ6ï¼šç‹€æ…‹æ›´æ–°"""
        start_time = time.time()
        context["workflow_trace"].append("æ›´æ–°ç‹€æ…‹ç®¡ç†")
        
        # æ›´æ–°æœƒè©±ç‹€æ…‹åˆ°Redisï¼ˆæ¨¡æ“¬ï¼‰
        state_update_request = StateManagementHandlerProtocol.create_state_transition_request(
            current_state="DataQuery",
            trigger="search_completed", 
            context=context
        )
        
        # æ¨¡æ“¬ç‹€æ…‹æ›´æ–°éŸ¿æ‡‰
        context["current_state"] = "User"  # å›åˆ°ç­‰å¾…ç”¨æˆ¶éŸ¿æ‡‰ç‹€æ…‹
        context["last_action"] = "product_recommendation_generated"
        context["workflow_trace"].append("ç‹€æ…‹æ›´æ–°å®Œæˆ")
        
        return ActionResult(
            status=ActionStatus.SUCCESS,
            data={"new_state": "User"},
            execution_time=time.time() - start_time
        )
    
    def _create_error_response(self, error_type: str, error_message: str) -> Dict[str, Any]:
        """å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰"""
        return {
            "success": False,
            "error": {
                "type": error_type,
                "message": error_message,
                "timestamp": time.time()
            },
            "response": f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚é‡åˆ°å•é¡Œï¼š{error_message}",
            "ui_elements": {
                "type": "error",
                "retry_available": True
            }
        }

# =====================================
# ä½¿ç”¨ç¯„ä¾‹
# =====================================

async def demo_mgfd_workflow():
    """æ¼”ç¤ºMGFDå®Œæ•´å·¥ä½œæµç¨‹"""
    workflow = MGFDWorkflowExample()
    
    print("=== MGFD SalesRAG ç³»çµ±å·¥ä½œæµç¨‹æ¼”ç¤º ===\n")
    
    # æ¨¡æ“¬ç”¨æˆ¶è«‹æ±‚
    user_input = "è«‹ä»‹ç´¹ç›®å‰æ–°å‡ºçš„ç­†é›»"
    session_id = str(uuid.uuid4())
    
    print(f"ç”¨æˆ¶è¼¸å…¥: {user_input}")
    print(f"æœƒè©±ID: {session_id}\n")
    
    # åŸ·è¡Œå·¥ä½œæµç¨‹
    result = await workflow.handle_user_request(user_input, session_id)
    
    if result["success"]:
        print("âœ… è™•ç†æˆåŠŸï¼")
        print(f"\nç³»çµ±éŸ¿æ‡‰:\n{result['response']}\n")
        
        print("ğŸ“Š UIå…ƒç´ :")
        ui_elements = result["ui_elements"]
        if ui_elements["type"] == "product_recommendation":
            print(f"- æ¨è–¦ç”¢å“æ•¸é‡: {len(ui_elements['products'])}")
            for product in ui_elements["products"]:
                print(f"  * {product['name']} (ç›¸ä¼¼åº¦: {product['similarity_score']:.1%})")
        
        print(f"\nğŸ” å·¥ä½œæµç¨‹è¿½è¹¤:")
        for i, step in enumerate(result["workflow_trace"], 1):
            print(f"  {i}. {step}")
    else:
        print("âŒ è™•ç†å¤±æ•—ï¼")
        print(f"éŒ¯èª¤: {result['error']['message']}")

# é‹è¡Œæ¼”ç¤º
if __name__ == "__main__":
    asyncio.run(demo_mgfd_workflow())