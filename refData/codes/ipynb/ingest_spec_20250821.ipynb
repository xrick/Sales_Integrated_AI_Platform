{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d74df-170a-48d2-b7ac-28c378dc2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/mapleleaf/LCJRepos/projects/mlinfo_kb_platform/refData/codes/ipynb/ingest_spec_20250821.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880fd725-55bb-434a-bd2a-eaa46f801247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mapleleaf/.conda/envs/salseragenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227151a6-ce66-44fa-a07a-850e13d279e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/mapleleaf/LCJRepos/projects/mlinfo_kb_platform/refData/codes/ipynb/ingest_spec_20250821.ipynb\n",
    "# ```python?code_reference&code_event_index=2\n",
    "\n",
    "\n",
    "# [Checklist Item 2: Define Constants]\n",
    "# --- Milvus Configuration ---\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"new_pc_v2\"\n",
    "\n",
    "# --- DuckDB Configuration ---\n",
    "DUCKDB_PATH = \"./nb_spec_v2.db\"\n",
    "\n",
    "# --- Embedding Model Configuration ---\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "DIMENSION = 384  # Dimension for all-MiniLM-L6-v2\n",
    "\n",
    "# --- Chunking Configuration ---\n",
    "PARENT_CHUNK_SIZE = 1024\n",
    "PARENT_CHUNK_OVERLAP = 128\n",
    "CHILD_CHUNK_SIZE = 256\n",
    "CHILD_CHUNK_OVERLAP = 32\n",
    "\n",
    "# [Checklist Item 3: Implement Milvus Connection and Setup Function]\n",
    "def setup_milvus_collection():\n",
    "    \"\"\"Sets up the Milvus connection and collection, recreating it if it exists.\"\"\"\n",
    "    print(\"Connecting to Milvus...\")\n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        print(f\"Collection '{COLLECTION_NAME}' already exists. Dropping it.\")\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "    print(f\"Creating collection '{COLLECTION_NAME}'...\")\n",
    "    #一般主鍵的加法# a.主鍵 (Primary Key)\n",
    "    fields = [\n",
    "        FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "    ]\n",
    "    #加入parent and child data\n",
    "    fields.append(FieldSchema(name=\"doc_id\", dtype=DataType.INT64))\n",
    "    # fields = [\n",
    "    #     # FieldSchema(name=\"uuid\", dtype=DataType.VARCHAR, is_primary=True, max_length=36),\n",
    "    #     FieldSchema(name=\"doc_id\", dtype=DataType.INT64),\n",
    "    #     FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    #     FieldSchema(name=\"parent_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    #     FieldSchema(name=\"chunk_text\", dtype=DataType.VARCHAR, max_length=1000),\n",
    "    #     FieldSchema(name=\"query_vector\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "    # ]\n",
    "    schema = CollectionSchema(fields, description=\"Parent-Child Chunk Collection\")\n",
    "    collection = Collection(COLLECTION_NAME, schema)\n",
    "\n",
    "    print(\"Creating index on 'query_vector'...\")\n",
    "    index_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"index_type\": \"HNSW\",\n",
    "        \"params\": {\"M\": 8, \"efConstruction\": 64}\n",
    "    }\n",
    "    collection.create_index(field_name=\"query_vector\", index_params=index_params)\n",
    "    collection.load()\n",
    "    return collection\n",
    "\n",
    "# [Checklist Item 4: Implement DuckDB Setup Function]\n",
    "def setup_duckdb_table():\n",
    "    \"\"\"Sets up the DuckDB connection and creates the documents table.\"\"\"\n",
    "    print(f\"Setting up DuckDB at '{DUCKDB_PATH}'...\")\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "            id BIGINT,\n",
    "            source VARCHAR,\n",
    "            content VARCHAR\n",
    "        );\n",
    "    \"\"\")\n",
    "    return con\n",
    "\n",
    "# [Checklist Item 5 & 6: Implement Data Loading, Processing, Iteration, and Chunking]\n",
    "def process_files(collection, db_con):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files, processes it, creates chunks, generates embeddings,\n",
    "    and inserts data into Milvus and DuckDB.\n",
    "    \"\"\"\n",
    "    print(f\"Loading sentence transformer model '{MODEL_NAME}'...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    # csv_files = [f for f in os.listdir('.') if f.startswith('data_') and f.endswith('.csv')]\n",
    "    # csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "    # print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "    csv_files = [f for f in os.listdir('./datasrc/') if f.endswith('.csv')]\n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "\n",
    "    doc_id_counter = 0\n",
    "    child_chunks_for_embedding = []\n",
    "    data_for_milvus = []\n",
    "\n",
    "    parent_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=PARENT_CHUNK_SIZE, chunk_overlap=PARENT_CHUNK_OVERLAP\n",
    "    )\n",
    "    child_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHILD_CHUNK_SIZE, chunk_overlap=CHILD_CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    for filename in tqdm(csv_files, desc=\"Processing Files\"):\n",
    "        try:\n",
    "            print(f\"current processing file:{filename}\")\n",
    "            df = pd.read_csv(filename)\n",
    "            # Combine all columns into a single text document for each row\n",
    "            df['combined_content'] = df.apply(\n",
    "                lambda row: ' | '.join(row.dropna().astype(str)), axis=1\n",
    "            )\n",
    "\n",
    "            for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Rows in {filename}\", leave=False):\n",
    "                content = row['combined_content']\n",
    "                if not content or content.isspace():\n",
    "                    continue\n",
    "\n",
    "                doc_id_counter += 1\n",
    "                source = filename\n",
    "\n",
    "                # Insert original content into DuckDB\n",
    "                db_con.execute(\"INSERT INTO documents VALUES (?, ?, ?)\", (doc_id_counter, source, content))\n",
    "\n",
    "                parent_chunks = parent_splitter.split_text(content)\n",
    "\n",
    "                for parent_chunk_text in parent_chunks:\n",
    "                    child_chunks = child_splitter.split_text(parent_chunk_text)\n",
    "                    for child_chunk_text in child_chunks:\n",
    "                        milvus_record = {\n",
    "                            \"uuid\": str(uuid.uuid4()),\n",
    "                            \"doc_id\": doc_id_counter,\n",
    "                            \"source\": source,\n",
    "                            \"parent_text\": parent_chunk_text,\n",
    "                            \"chunk_text\": child_chunk_text\n",
    "                        }\n",
    "                        data_for_milvus.append(milvus_record)\n",
    "                        child_chunks_for_embedding.append(child_chunk_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "        print(\"success\")\n",
    "    # [Checklist Item 7: Implement Batch Embedding]\n",
    "#     if not child_chunks_for_embedding:\n",
    "#         print(\"No text chunks found to embed.\")\n",
    "#         return 0, 0\n",
    "\n",
    "#     print(f\"\\nEmbedding {len(child_chunks_for_embedding)} child chunks in a batch...\")\n",
    "#     embeddings = model.encode(\n",
    "#         child_chunks_for_embedding,\n",
    "#         show_progress_bar=True,\n",
    "#         batch_size=32\n",
    "#     )\n",
    "\n",
    "#     # [Checklist Item 8: Implement Milvus Data Insertion]\n",
    "#     for i, record in enumerate(data_for_milvus):\n",
    "#         record['query_vector'] = embeddings[i]\n",
    "\n",
    "#     print(\"Preparing data for Milvus insertion...\")\n",
    "#     # Reformat data for pymilvus insert\n",
    "#     entities = [\n",
    "#         [rec[\"uuid\"] for rec in data_for_milvus],\n",
    "#         [rec[\"doc_id\"] for rec in data_for_milvus],\n",
    "#         [rec[\"source\"] for rec in data_for_milvus],\n",
    "#         [rec[\"parent_text\"] for rec in data_for_milvus],\n",
    "#         [rec[\"chunk_text\"] for rec in data_for_milvus],\n",
    "#         [rec[\"query_vector\"] for rec in data_for_milvus],\n",
    "#     ]\n",
    "\n",
    "#     print(f\"Inserting {len(data_for_milvus)} records into Milvus collection...\")\n",
    "#     # Insert in batches\n",
    "#     batch_size = 1000\n",
    "#     for i in tqdm(range(0, len(data_for_milvus), batch_size), desc=\"Inserting into Milvus\"):\n",
    "#         batch_entities = [field[i:i + batch_size] for field in entities]\n",
    "#         collection.insert(batch_entities)\n",
    "\n",
    "#     print(\"Flushing Milvus collection...\")\n",
    "#     collection.flush()\n",
    "\n",
    "#     return doc_id_counter, len(data_for_milvus)\n",
    "\n",
    "\n",
    "# # [Checklist Item 9 & 10: Finalize Script and Cleanup]\n",
    "# if __name__ == \"__main__\":\n",
    "#     milvus_collection = None\n",
    "#     duckdb_connection = None\n",
    "#     try:\n",
    "#         milvus_collection = setup_milvus_collection()\n",
    "#         duckdb_connection = setup_duckdb_table()\n",
    "#         total_docs, total_chunks = process_files(milvus_collection, duckdb_connection)\n",
    "#         print(\"\\n--- Processing Complete ---\")\n",
    "#         print(f\"Total original documents processed: {total_docs}\")\n",
    "#         print(f\"Total child chunks created and inserted: {total_chunks}\")\n",
    "#         print(f\"Data successfully saved to DuckDB ('{DUCKDB_PATH}') and Milvus ('{COLLECTION_NAME}').\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nAn error occurred: {e}\")\n",
    "#     finally:\n",
    "#         if duckdb_connection:\n",
    "#             duckdb_connection.close()\n",
    "#             print(\"DuckDB connection closed.\")\n",
    "#         if utility.has_collection(COLLECTION_NAME):\n",
    "#             connections.disconnect(\"default\")\n",
    "#             print(\"Milvus connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df622205-95b7-43d0-a1a6-7a0efc254a6f",
   "metadata": {},
   "source": [
    "### functions unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4831d-76fd-4e96-8eca-515b050c1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01879b86-e3b9-47e5-82b3-ddce8fad3e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model 'all-MiniLM-L6-v2'...\n",
      "Found 19 CSV files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 15890.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current processing file:data_8329.csv\n",
      "Error processing file data_8329.csv: [Errno 2] No such file or directory: 'data_8329.csv'\n",
      "success\n",
      "current processing file:data_938.csv\n",
      "Error processing file data_938.csv: [Errno 2] No such file or directory: 'data_938.csv'\n",
      "success\n",
      "current processing file:data_529.csv\n",
      "Error processing file data_529.csv: [Errno 2] No such file or directory: 'data_529.csv'\n",
      "success\n",
      "current processing file:data_835.csv\n",
      "Error processing file data_835.csv: [Errno 2] No such file or directory: 'data_835.csv'\n",
      "success\n",
      "current processing file:data_27.csv\n",
      "Error processing file data_27.csv: [Errno 2] No such file or directory: 'data_27.csv'\n",
      "success\n",
      "current processing file:AC01_result.csv\n",
      "Error processing file AC01_result.csv: [Errno 2] No such file or directory: 'AC01_result.csv'\n",
      "success\n",
      "current processing file:data_17.csv\n",
      "Error processing file data_17.csv: [Errno 2] No such file or directory: 'data_17.csv'\n",
      "success\n",
      "current processing file:data_839.csv\n",
      "Error processing file data_839.csv: [Errno 2] No such file or directory: 'data_839.csv'\n",
      "success\n",
      "current processing file:data_918.csv\n",
      "Error processing file data_918.csv: [Errno 2] No such file or directory: 'data_918.csv'\n",
      "success\n",
      "current processing file:data_819.csv\n",
      "Error processing file data_819.csv: [Errno 2] No such file or directory: 'data_819.csv'\n",
      "success\n",
      "current processing file:data_829.csv\n",
      "Error processing file data_829.csv: [Errno 2] No such file or directory: 'data_829.csv'\n",
      "success\n",
      "current processing file:data_928.csv\n",
      "Error processing file data_928.csv: [Errno 2] No such file or directory: 'data_928.csv'\n",
      "success\n",
      "current processing file:data_531.csv\n",
      "Error processing file data_531.csv: [Errno 2] No such file or directory: 'data_531.csv'\n",
      "success\n",
      "current processing file:data_656.csv\n",
      "Error processing file data_656.csv: [Errno 2] No such file or directory: 'data_656.csv'\n",
      "success\n",
      "current processing file:data_960.csv\n",
      "Error processing file data_960.csv: [Errno 2] No such file or directory: 'data_960.csv'\n",
      "success\n",
      "current processing file:data_958.csv\n",
      "Error processing file data_958.csv: [Errno 2] No such file or directory: 'data_958.csv'\n",
      "success\n",
      "current processing file:data_326.csv\n",
      "Error processing file data_326.csv: [Errno 2] No such file or directory: 'data_326.csv'\n",
      "success\n",
      "current processing file:data_525.csv\n",
      "Error processing file data_525.csv: [Errno 2] No such file or directory: 'data_525.csv'\n",
      "success\n",
      "current processing file:data_728.csv\n",
      "Error processing file data_728.csv: [Errno 2] No such file or directory: 'data_728.csv'\n",
      "success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_files(COLLECTION_NAME,DUCKDB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758f9e21-2583-4614-94d6-4490d249b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 CSV files to process.\n"
     ]
    }
   ],
   "source": [
    "# csv_files = [f for f in os.listdir('../../../data/raw/EM_New TTL_241104_AllTransformedToGoogleSheet/') if f.endswith('.csv')]\n",
    "# print(f\"Found {len(csv_files)} CSV files to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9abca868-7f47-4606-8511-7ae9d7383d91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionNotExistException",
     "evalue": "<ConnectionNotExistException: (code=1, message=should create connection first.)>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionNotExistException\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymilvus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Collection\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 連接到 Milvus 服務\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ...（根據你的連線方式填寫 host/port 等）...\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 載入集合\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m collection = \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 查詢集合資訊（例如 schema、row count）\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m總紀錄數: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection.num_entities\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/salseragenv/lib/python3.11/site-packages/pymilvus/orm/collection.py:117\u001b[39m, in \u001b[36mCollection.__init__\u001b[39m\u001b[34m(self, name, schema, using, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m._kwargs = kwargs\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._num_shards = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m conn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m has = conn.has_collection(\u001b[38;5;28mself\u001b[39m._name, **kwargs)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/salseragenv/lib/python3.11/site-packages/pymilvus/orm/collection.py:173\u001b[39m, in \u001b[36mCollection._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnections\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fetch_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_using\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/salseragenv/lib/python3.11/site-packages/pymilvus/orm/connections.py:558\u001b[39m, in \u001b[36mConnections._fetch_handler\u001b[39m\u001b[34m(self, alias)\u001b[39m\n\u001b[32m    556\u001b[39m conn = \u001b[38;5;28mself\u001b[39m._connected_alias.get(alias, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotExistException(message=ExceptionsMessage.ConnectFirst)\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[31mConnectionNotExistException\u001b[39m: <ConnectionNotExistException: (code=1, message=should create connection first.)>"
     ]
    }
   ],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "# 連接到 Milvus 服務\n",
    "# ...（根據你的連線方式填寫 host/port 等）...\n",
    "# --- 2. 處理並存入非結構化資料 (Milvus) ---\n",
    "    print(\"\\n--- 正在處理文本資料並存入 Milvus ---\")\n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        print(f\"找到舊的 Milvus Collection '{COLLECTION_NAME}'，正在刪除...\")\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "# 載入集合\n",
    "collection = Collection(name=COLLECTION_NAME)\n",
    "\n",
    "# 查詢集合資訊（例如 schema、row count）\n",
    "print(f\"總紀錄數: {collection.num_entities}\")\n",
    "print(f\"結構: {collection.schema}\")\n",
    "\n",
    "# 查詢部分數據（例如查前10條）\n",
    "results = collection.query(\n",
    "    expr=None,   # 無條件則查全部\n",
    "    output_fields=[\"欄位1\", \"欄位2\"],   # 需填你關心的欄位；如未知可用 collection.schema\n",
    "    limit=10\n",
    ")\n",
    "for item in results:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62249af6-ca4e-470d-8efa-88362744cfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
