#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ LLM æä¾›å•†å¯ç”¨æ€§å’ŒåŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from libs.mgfd_cursor.llm_manager import MGFDLLMManager

def test_llm_provider_imports():
    """æ¸¬è©¦ LLM æä¾›å•†å°å…¥ç‹€æ…‹"""
    print("=== æ¸¬è©¦ LLM æä¾›å•†å°å…¥ç‹€æ…‹ ===\n")
    
    try:
        from libs.mgfd_cursor.llm_manager import (
            OLLAMA_AVAILABLE, OPENAI_AVAILABLE, ANTHROPIC_AVAILABLE, LANGCHAIN_AVAILABLE
        )
        
        print(f"Ollama å¯ç”¨: {'âœ“' if OLLAMA_AVAILABLE else 'âœ—'}")
        print(f"OpenAI å¯ç”¨: {'âœ“' if OPENAI_AVAILABLE else 'âœ—'}")
        print(f"Anthropic å¯ç”¨: {'âœ“' if ANTHROPIC_AVAILABLE else 'âœ—'}")
        print(f"LangChain æ•´é«”å¯ç”¨: {'âœ“' if LANGCHAIN_AVAILABLE else 'âœ—'}")
        
        return LANGCHAIN_AVAILABLE
        
    except Exception as e:
        print(f"âœ— å°å…¥æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_ollama_provider():
    """æ¸¬è©¦ Ollama æä¾›å•†"""
    print("\n=== æ¸¬è©¦ Ollama æä¾›å•† ===")
    
    try:
        # å‰µå»º Ollama LLM ç®¡ç†å™¨
        llm_manager = MGFDLLMManager(
            provider="ollama",
            model_name="deepseek-r1:7b",
            temperature=0.1
        )
        
        # æª¢æŸ¥ç‹€æ…‹
        status = llm_manager.get_status()
        print(f"Provider: {status['provider']}")
        print(f"Model: {status['model_name']}")
        print(f"LLM Available: {'âœ“' if status['llm_available'] else 'âœ—'}")
        print(f"Provider Availability: {status['provider_availability']}")
        
        # æ¸¬è©¦èª¿ç”¨
        if status['llm_available']:
            try:
                response = llm_manager.invoke("è«‹ç”¨ä¸€å¥è©±å›æ‡‰: ä½ å¥½")
                print(f"âœ“ Ollama å›æ‡‰: {response[:100]}...")
                return True
            except Exception as e:
                print(f"âœ— Ollama èª¿ç”¨å¤±æ•—: {e}")
                return False
        else:
            print("âœ— Ollama LLM ä¸å¯ç”¨")
            return False
            
    except Exception as e:
        print(f"âœ— Ollama æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_openai_provider():
    """æ¸¬è©¦ OpenAI æä¾›å•† (ä¸éœ€è¦ API keyï¼Œåªæ¸¬è©¦å°å…¥)"""
    print("\n=== æ¸¬è©¦ OpenAI æä¾›å•† (å°å…¥æ¸¬è©¦) ===")
    
    try:
        # å‰µå»º OpenAI LLM ç®¡ç†å™¨ (ä¸æä¾› API keyï¼Œé æœŸæœƒå¤±æ•—ä½†å°å…¥æˆåŠŸ)
        llm_manager = MGFDLLMManager(
            provider="openai",
            model_name="gpt-3.5-turbo",
            temperature=0.1
        )
        
        # æª¢æŸ¥ç‹€æ…‹
        status = llm_manager.get_status()
        print(f"Provider: {status['provider']}")
        print(f"Model: {status['model_name']}")
        print(f"OpenAI å¥—ä»¶å¯ç”¨: {'âœ“' if status['provider_availability']['openai'] else 'âœ—'}")
        
        # ä¸æ¸¬è©¦å¯¦éš›èª¿ç”¨ï¼Œå› ç‚ºéœ€è¦ API key
        print("ğŸ“ æ³¨æ„: å¯¦éš›èª¿ç”¨éœ€è¦ OpenAI API key")
        return status['provider_availability']['openai']
        
    except Exception as e:
        print(f"âœ— OpenAI æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_anthropic_provider():
    """æ¸¬è©¦ Anthropic æä¾›å•† (ä¸éœ€è¦ API keyï¼Œåªæ¸¬è©¦å°å…¥)"""
    print("\n=== æ¸¬è©¦ Anthropic æä¾›å•† (å°å…¥æ¸¬è©¦) ===")
    
    try:
        # å‰µå»º Anthropic LLM ç®¡ç†å™¨ (ä¸æä¾› API keyï¼Œé æœŸæœƒå¤±æ•—ä½†å°å…¥æˆåŠŸ)
        llm_manager = MGFDLLMManager(
            provider="anthropic",
            model_name="claude-3-sonnet-20240229",
            temperature=0.1
        )
        
        # æª¢æŸ¥ç‹€æ…‹
        status = llm_manager.get_status()
        print(f"Provider: {status['provider']}")
        print(f"Model: {status['model_name']}")
        print(f"Anthropic å¥—ä»¶å¯ç”¨: {'âœ“' if status['provider_availability']['anthropic'] else 'âœ—'}")
        
        # ä¸æ¸¬è©¦å¯¦éš›èª¿ç”¨ï¼Œå› ç‚ºéœ€è¦ API key
        print("ğŸ“ æ³¨æ„: å¯¦éš›èª¿ç”¨éœ€è¦ Anthropic API key")
        return status['provider_availability']['anthropic']
        
    except Exception as e:
        print(f"âœ— Anthropic æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_mock_fallback():
    """æ¸¬è©¦æ¨¡æ“¬å›æ‡‰æ©Ÿåˆ¶"""
    print("\n=== æ¸¬è©¦æ¨¡æ“¬å›æ‡‰æ©Ÿåˆ¶ ===")
    
    try:
        # å‰µå»ºä¸€å€‹ä¸å­˜åœ¨çš„æä¾›å•†ä¾†è§¸ç™¼æ¨¡æ“¬æ¨¡å¼
        llm_manager = MGFDLLMManager(provider="invalid_provider")
        
        # æ¸¬è©¦æ¨¡æ“¬å›æ‡‰
        response = llm_manager.invoke("æ¸¬è©¦æç¤ºè©")
        print(f"âœ“ æ¨¡æ“¬å›æ‡‰: {response}")
        
        # æ¸¬è©¦æ§½ä½åˆ†ææ¨¡æ“¬
        mock_slot_response = llm_manager.classify_slot("åˆ†ææ§½ä½çš„æç¤ºè©")
        print(f"âœ“ æ¨¡æ“¬æ§½ä½åˆ†é¡: {mock_slot_response[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡æ“¬å›æ‡‰æ¸¬è©¦å¤±æ•—: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ” LLM æä¾›å•†ç¶œåˆæ¸¬è©¦\n")
    
    # æ¸¬è©¦çµæœæ”¶é›†
    results = {
        "imports": False,
        "ollama": False,
        "openai": False,
        "anthropic": False,
        "mock_fallback": False
    }
    
    # åŸ·è¡Œå„é …æ¸¬è©¦
    results["imports"] = test_llm_provider_imports()
    results["ollama"] = test_ollama_provider()
    results["openai"] = test_openai_provider()
    results["anthropic"] = test_anthropic_provider()
    results["mock_fallback"] = test_mock_fallback()
    
    # ç¸½çµå ±å‘Š
    print("\n" + "="*50)
    print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ")
    print("="*50)
    
    for test_name, result in results.items():
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{test_name.ljust(15)}: {status}")
    
    # æ•´é«”è©•ä¼°
    critical_tests = ["imports", "ollama", "mock_fallback"]
    critical_passed = all(results[test] for test in critical_tests)
    
    print(f"\nğŸ¯ é—œéµåŠŸèƒ½ç‹€æ…‹: {'âœ… æ­£å¸¸' if critical_passed else 'âŒ ç•°å¸¸'}")
    
    if critical_passed:
        print("ğŸ‰ LLM ç³»çµ±å·²æº–å‚™å°±ç·’ï¼")
        print("ğŸ’¡ Ollama å¯æ­£å¸¸ä½¿ç”¨ï¼ŒOpenAI/Anthropic éœ€è¦ç›¸æ‡‰ API keys")
    else:
        print("âš ï¸  LLM ç³»çµ±å­˜åœ¨å•é¡Œï¼Œè«‹æª¢æŸ¥é…ç½®")
    
    return critical_passed

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)